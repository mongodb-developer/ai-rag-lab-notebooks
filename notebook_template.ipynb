{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mongodb-developer/ai-agents-lab-notebooks/blob/main/notebook_template.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Lab Documentation and Solutions](https://img.shields.io/badge/Lab%20Documentation%20and%20Solutions-purple)](https://mongodb-developer.github.io/rag-lab/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Install libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU pymongo langchain langchain-community fireworks-ai bs4 tiktoken sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Setup prerequisites\n",
    "\n",
    "Replace:\n",
    "\n",
    "- `<CODE_BLOCK_1>` with your **MongoDB connection string**\n",
    "- `<CODE_BLOCK_2>` with your **Fireworks API key**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain the quotes (\"\") when pasting the URI\n",
    "MONGODB_URI = \"<CODE_BLOCK_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain the quotes (\"\") when pasting the API key\n",
    "os.environ[\"FIREWORKS_API_KEY\"] = \"<CODE_BLOCK_2>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    [\n",
    "        \"https://www.mongodb.com/developer/products/atlas/choose-embedding-model-rag/\",\n",
    "        \"https://www.mongodb.com/developer/products/atlas/evaluate-llm-applications-rag/\",\n",
    "        \"https://www.mongodb.com/developer/products/atlas/choosing-chunking-strategy-rag/\",\n",
    "        \"https://www.mongodb.com/developer/products/atlas/gemma-mongodb-huggingface-rag/\",\n",
    "    ]\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of documents created\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview a document\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the `page_content` attribute of the Document object\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the metadata attribute of the Document object\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Chunk up the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/split_by_token/#tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `RecursiveCharacterTextSplitter` text splitter with the `cl100k_base` encoding\n",
    "# For text data, you typically want to keep 1-2 paragraphs (~200 tokens) in a single chunk\n",
    "# Chunk overlap of 15-20% of the chunk size is recommended\n",
    "text_splitter = <CODE_BLOCK_3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split `docs` using the appropriate method of the `RecursiveCharacterTextSplitter` class\n",
    "# NOTE: `docs` is a list of LangChain documents\n",
    "split_docs = <CODE_BLOCK_4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the length of the list of chunked documents is greater than the length of `docs`\n",
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a Python list comprehension to convert each LangChain Document object in `split_docs` to a Python dictionary.\n",
    "# Use the `.dict()` method on each Document object.\n",
    "split_docs = [<CODE_BLOCK_5>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview one of the items in split_docs- ensure that it is a Python dictionary\n",
    "split_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Generate embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Mixedbread AI's `mxbai-embed-large-v1` model using the Sentence Transformers library\n",
    "embedding_model = <CODE_BLOCK_6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that takes a piece of text (`text`) as input, embeds it using the `embedding_model` instantiated above and returns the embedding as a list\n",
    "# NOTE: An array can be converted to a list using the `tolist()` method\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Generate the embedding for a piece of text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to embed.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: Embedding of the text as a list.\n",
    "    \"\"\"\n",
    "    <CODE_BLOCK_7>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to add an `embedding` field to each dictionary in `split_docs`\n",
    "# The `embedding` field should correspond to the embedding of the value of the `page_content` field\n",
    "# Use the `get_embedding` function defined above to generate the embedding\n",
    "# NOTE: Append the updated dictionaries to `embedded_docs` initialized above.\n",
    "<CODE_BLOCK_8>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Ingest data into MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö https://pymongo.readthedocs.io/en/stable/api/pymongo/mongo_client.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a MongoDB Python client\n",
    "mongo_client = <CODE_BLOCK_9>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the database -- Change if needed or leave as is\n",
    "DB_NAME = \"mongodb_rag_lab\"\n",
    "# Name of the collection -- Change if needed or leave as is\n",
    "COLLECTION_NAME = \"knowledge_base\"\n",
    "# Name of the vector search index -- Change if needed or leave as is\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö https://pymongo.readthedocs.io/en/stable/tutorial.html#getting-a-collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the collection defined above using the MongoDB client\n",
    "collection = <CODE_BLOCK_10>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulk delete all existing records from the collection defined above -- should be a one-liner\n",
    "<CODE_BLOCK_11>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulk insert `embedded_docs` into the collection defined above -- should be a one-liner\n",
    "<CODE_BLOCK_12>\n",
    "\n",
    "print(\"Data ingestion into MongoDB completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Create a vector search index\n",
    "\n",
    "Follow the instructions in the documentation to create a Vector Search index in the Atlas UI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Perform semantic search on your data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a vector search function\n",
    "\n",
    "üìö https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#fields\n",
    "\n",
    "üìö https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples (Refer to the \"Basic Example\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to retrieve relevant documents for a user query using vector search\n",
    "def vector_search(user_query: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents for a user query using vector search.\n",
    "\n",
    "    Args:\n",
    "    user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of matching documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate embedding for the `user_query` using the `get_embedding` function defined in Step 2\n",
    "    <CODE_BLOCK_13>\n",
    "\n",
    "    # Define an aggregation pipeline consisting of a $vectorSearch stage, followed by a $project stage\n",
    "    # Set the number of candidates to 150 and only return the top 5 documents from the vector search\n",
    "    # In the $project stage, exclude the `_id` field and include only the `page_content` field and `vectorSearchScore`\n",
    "    # NOTE: Use variables defined previously for the `index`, `queryVector` and `path` fields in the $vectorSearch stage\n",
    "    pipeline = <CODE_BLOCK_14>\n",
    "\n",
    "    # Execute the aggregation `pipeline`` and store the results in `results`\n",
    "    results = <CODE_BLOCK_15>\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run vector search queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_search(\n",
    "    \"What are the important considerations while choosing an embedding model?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_search(\"How to choose a chunking strategy for RAG?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü¶π‚Äç‚ôÄÔ∏è Combine pre-filtering with vector search\n",
    "\n",
    "üìö https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-type/#about-the-filter-type\n",
    "\n",
    "üìö https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples (Refer to the \"Filter Example\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter for documents where the language is `en`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the vector search index definition to include the `metadata.language` field as a `filter` field\n",
    "<CODE_BLOCK_16>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the $vectorSearch stage of the aggregation pipeline defined previously to include a filter for documents where the `metadata.language` field has the value `en`\n",
    "<CODE_BLOCK_17>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter on documents where the language is `en` and type is `Document`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the vector search index definition to include the `metadata.language` and `type` fields as `filter` fields\n",
    "<CODE_BLOCK_18>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the $vectorSearch stage of the aggregation pipeline defined previously to include a filter for documents where\n",
    "# the `metadata.language` field has the value `en`\n",
    "# AND\n",
    "# the `type` field has the value `Document`\n",
    "<CODE_BLOCK_19>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Build a RAG application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate a chat model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fireworks.client import Fireworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the Fireworks AI client and the model string\n",
    "fw_client = Fireworks()\n",
    "model = \"accounts/fireworks/models/llama-v3-8b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to create the chat prompt\n",
    "\n",
    "üìö https://docs.python.org/3/library/stdtypes.html#str.join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the user prompt for our RAG application\n",
    "def create_prompt(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a chat prompt that includes the user query and retrieved context.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "\n",
    "    Returns:\n",
    "        str: The chat prompt string.\n",
    "    \"\"\"\n",
    "    # Retrieve the most relevant documents for the `user_query` using the `vector_search` function\n",
    "    context = <CODE_BLOCK_20>\n",
    "    # Join the retrieved documents into a single string, where each document is separated by two new lines (\"\\n\\n\")\n",
    "    # NOTE: Extract only the `page_content` field from the documents\n",
    "    context = <CODE_BLOCK_21>\n",
    "    # Prompt consisting of the question and relevant context to answer it\n",
    "    prompt = f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\\n\\nQuestion:{user_query}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to answer user queries\n",
    "\n",
    "üìö https://docs.fireworks.ai/guides/querying-text-models#chat-completions-api\n",
    "\n",
    "üìö https://docs.fireworks.ai/api-reference/post-completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to answer user queries using Fireworks' Chat Completion API\n",
    "def generate_answer(user_query: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate an answer to the user query.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "    \"\"\"\n",
    "    # Use the `create_prompt` function above to fill in the `content` field in the chat message\n",
    "    # Set the `temperature` parameter to 0 to get more deterministic responses\n",
    "    # Print the final answer\n",
    "    <CODE_BLOCK_22>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the RAG application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(\n",
    "    \"What are the important considerations while choosing an embedding model?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(\"What did I just ask you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü¶π‚Äç‚ôÄÔ∏è Return streaming responses\n",
    "\n",
    "üìö https://docs.fireworks.ai/guides/querying-text-models#streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to answer user queries in streaming mode using Fireworks' Chat Completion API \n",
    "def generate_answer(user_query: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate an answer to the user query.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "    \"\"\"\n",
    "    # Use the `create_prompt` function above to fill in the `content` field in the chat message\n",
    "    # Set the `temperature` parameter to 0 to get more deterministic responses\n",
    "    # Set the `stream` parameter to True\n",
    "    response = <CODE_BLOCK_23>\n",
    "\n",
    "    # Iterate through the `response` generator and print the results as they are generated\n",
    "    <CODE_BLOCK_24>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(\n",
    "    \"What are the important considerations while choosing an embedding model?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Add memory to the RAG application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_collection = mongo_client[DB_NAME][\"chat_history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index on the key `session_id` for the `history_collection` collection\n",
    "<CODE_BLOCK_25>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to store chat messages in MongoDB\n",
    "\n",
    "üìö https://docs.python.org/3/library/datetime.html#datetime.datetime.nowhttps://docs.python.org/3/library/datetime.html#datetime.datetime.now\n",
    "\n",
    "üìö https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.insert_one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_chat_message(session_id: str, role: str, content: str) -> None:\n",
    "    \"\"\"\n",
    "    Store a chat message in a MongoDB collection.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID of the message.\n",
    "        role (str): Role for the message. One of `system`, `user` or `assistant`.\n",
    "        content (str): Content of the message.\n",
    "    \"\"\"\n",
    "    # Create a message object with `session_id`, `role`, `content` and `timestamp` fields\n",
    "    # `timestamp` should be set the current timestamp\n",
    "    message = <CODE_BLOCK_26>\n",
    "    # Insert the `message` into the `history_collection` collection\n",
    "    <CODE_BLOCK_27>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to retrieve chat history from MongoDB\n",
    "\n",
    "üìö https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.find\n",
    "\n",
    "üìö https://pymongo.readthedocs.io/en/stable/api/pymongo/cursor.html#pymongo.cursor.Cursor.sort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_session_history(session_id: str) -> List:\n",
    "    \"\"\"\n",
    "    Retrieve chat message history for a particular session.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID to retrieve chat message history for.\n",
    "\n",
    "    Returns:\n",
    "        List: List of chat messages.\n",
    "    \"\"\"\n",
    "    # Query the `history_collection` collection for documents where the \"session_id\" field has the value of the input `session_id`\n",
    "    # Sort the results in increasing order of the values in `timestamp` field\n",
    "    cursor =  <CODE_BLOCK_28>\n",
    "\n",
    "    if cursor:\n",
    "        # Write a list comprehension to iterate through the cursor and extract the `role` and `content` field from each entry\n",
    "        # Then format each entry as: {\"role\": <role_value>, \"content\": <content_value>}\n",
    "        messages = [<CODE_BLOCK_29>]\n",
    "    else:\n",
    "        # If cursor is empty, return an empty list\n",
    "        messages = []\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle chat history in the `generate_answer` function\n",
    "\n",
    "üìö https://docs.python.org/3/tutorial/datastructures.html\n",
    "\n",
    "üìö https://docs.fireworks.ai/guides/querying-text-models#chat-completions-api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(session_id: str, user_query: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate an answer to the user's query taking chat history into account.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Session ID to retrieve chat history for.\n",
    "        user_query (str): The user's query string.\n",
    "    \"\"\"\n",
    "    # Initialize list of messages to pass to the chat completion model\n",
    "    messages = []\n",
    "\n",
    "    # Retrieve documents relevant to the user query and convert them to a single string\n",
    "    context = vector_search(user_query)\n",
    "    context = \"\\n\\n\".join([d.get(\"page_content\", \"\") for d in context])\n",
    "    # Create a system prompt containing the retrieved context\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\",\n",
    "    }\n",
    "    # Append the system prompt to the `messages` list\n",
    "    messages.append(system_message)\n",
    "\n",
    "    # Use the `retrieve_session_history` function to retrieve message history from MongoDB for the session ID `session_id` \n",
    "    # And add all messages in the message history to the `messages` list \n",
    "    <CODE_BLOCK_30>\n",
    "\n",
    "    # Format the user message in the format {\"role\": <role_value>, \"content\": <content_value>}\n",
    "    # The role value for user messages must be \"user\"\n",
    "    # And append the user message to the `messages` list\n",
    "    <CODE_BLOCK_31>\n",
    "\n",
    "    # Call the chat completions API \n",
    "    response = fw_client.chat.completions.create(model=model, messages=messages)\n",
    "\n",
    "    # Extract the answer from the API response\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    # Use the `store_chat_message` function to store the user message and also the generated answer in the message history collection\n",
    "    # The role value for user messages is \"user\", and \"assistant\" for the generated answer\n",
    "    <CODE_BLOCK_32>\n",
    "\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(\n",
    "    session_id=\"1\",\n",
    "    user_query=\"What are the important considerations while choosing an embedding model?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(\n",
    "    session_id=\"1\",\n",
    "    user_query=\"What did I just ask you?\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
